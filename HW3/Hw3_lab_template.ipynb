{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 3: Lab\n",
        "### 10 points total\n",
        "### Version 1.0"
      ],
      "metadata": {
        "id": "WLzS4q4x_CXH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smriti Srikanth (ssrikan2), Hannah Qu (hqu6)"
      ],
      "metadata": {
        "id": "eNRCplBw_IbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import string\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'-\"\n",
        "n_letters = len(all_letters)\n",
        "# Find letter index from all_letters, e.g. \"a\" = 0\n",
        "def letterToIndex(letter):\n",
        "    return all_letters.find(letter)\n",
        "\n",
        "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
        "def letterToTensor(letter):\n",
        "    tensor = torch.zeros(1, n_letters)\n",
        "    tensor[0][letterToIndex(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# Turn a line into a <line_length x 1 x n_letters>,\n",
        "# or an array of one-hot letter vectors\n",
        "def nameToTensor(name):\n",
        "    tensor = torch.zeros(len(name), 1, n_letters)\n",
        "    for li, letter in enumerate(name):\n",
        "        tensor[li][0][letterToIndex(letter)] = 1\n",
        "    return tensor"
      ],
      "metadata": {
        "id": "EHuDp_vViF2W"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "#from utils import *\n",
        "import random\n",
        "import time\n",
        "import pdb\n",
        "import json\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "n_letters=58\n",
        "n_categories=18\n",
        "n_hidden = 128\n",
        "n_epochs = 100\n",
        "print_every = 5000\n",
        "plot_every = 1000\n",
        "learning_rate = 0.0001 # If you set this too high, it might explode. If too low, it might not learn\n",
        "\n",
        "class RNN(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_shape (int): size of the 1-hot embeddings for each character (this will be 58)\n",
        "            hidden_layer_width (int): number of nodes in the single hidden layer within the model\n",
        "            n_classes (int): number of output classes\n",
        "        \"\"\"\n",
        "        super(RNN, self).__init__()\n",
        "        ### TODO Implement the network architecture\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.batch_size = 1\n",
        "\n",
        "        self.i2h = torch.nn.Linear(input_size, hidden_size, bias=True)\n",
        "        self.h2h = torch.nn.Linear(hidden_size, hidden_size)\n",
        "        self.h2o = torch.nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        \"\"\"Forward function accepts tensor of input data, returns tensor of output data.\n",
        "        Modules defined in constructor are used, along with arbitrary operators on tensors\n",
        "        \"\"\"\n",
        "        \n",
        "        ### TODO Implement the forward function\n",
        "\n",
        "        hidden = self.i2h(input)\n",
        "        hidden = self.h2h(hidden+input)\n",
        "        g = torch.nn.Softmax()\n",
        "        output = g(self.h2o(hidden))\n",
        "        \n",
        "\n",
        "        #your function will return the output y(t) and hidden h(t) from equation 1 in the docs\n",
        "        return output, hidden \n",
        "\n",
        "    def initHidden(self):\n",
        "        \"\"\"\n",
        "        This function initializes the first hidden state of the RNN as a zero tensor.\n",
        "        \"\"\"\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "def get_xy_pairs(names):\n",
        "    #TODO \n",
        "    #process the names dict and convert into a list of (x,y) pairs. x is a 1-hot tensor of size (num_characters_in_name, 1, n_letters)\n",
        "    #y is a scalar representing the category of the language, there are 18 languages, assign an index between 0-17 to each language and y represents this index.\n",
        "    #you may make use of the nameToTensor() function in the utils.py file to help you with this function\n",
        "\n",
        "    list_of_pairs = []\n",
        "    categories = {}\n",
        "    y = 0\n",
        "\n",
        "    for language in names.keys():\n",
        "      categories[language] = y\n",
        "      for name in language:\n",
        "        x = nameToTensor(name)\n",
        "        list_of_pairs.append([x,y])\n",
        "      y = y+1\n",
        "\n",
        "    return list_of_pairs\n",
        "\n",
        "def create_train_and_test_set(list_of_pairs):\n",
        "    #TODO \n",
        "    #process the list of (x,y) pairs and split them 80-20 into train and test set\n",
        "    #train_x is a list of name embeddings each of size (num_characters_in_name, 1, n_letters), train_y is the correponding list of language category index. Same for test_x and test_y\n",
        "\n",
        "    random.seed(0)\n",
        "\n",
        "    train_x = []\n",
        "    train_y = []\n",
        "    test_x = []\n",
        "    test_y = []\n",
        "\n",
        "    for pair in list_of_pairs:\n",
        "      if random.randrange(0,10) > 1:\n",
        "        train_x.append(pair[0])\n",
        "        train_y.append(pair[1])\n",
        "      else:\n",
        "        test_x.append(pair[0])\n",
        "        test_y.append(pair[1])\n",
        "\n",
        "    return train_x, train_y, test_x, test_y\n",
        "\n",
        "rnn = RNN(n_letters, n_hidden, n_categories)\n",
        "optimizer = torch.optim.SGD(rnn.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.NLLLoss()\n",
        "crossEntropyLoss = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train(train_x, train_y):\n",
        "    \"\"\"train_x and train_y are lists with names and correspoonding labels\"\"\"\n",
        "    loss = 0\n",
        "    cel = []\n",
        "    for x, y in zip(train_x, train_y):\n",
        "        hidden = rnn.initHidden()\n",
        "        for i in range(x.size()[0]):\n",
        "            output, hidden = rnn(x[i], hidden)\n",
        "\n",
        "        loss += criterion(torch.log(output), torch.tensor(y).unsqueeze(0)) #the unsqueeze converts the scalar y to a 1D tensor\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "def test(train_x, train_y):\n",
        "    \"\"\"train_x and train_y are lists with names and correspoonding labels\"\"\"\n",
        "    loss = 0\n",
        "    with torch.no_grad(): \n",
        "        for x, y in zip(train_x, train_y):\n",
        "            hidden = rnn.initHidden()\n",
        "            for i in range(x.size()[0]):\n",
        "                output, hidden = rnn(x[i], hidden)\n",
        "\n",
        "            loss += crossEntropyLoss(output, torch.tensor(y).unsqueeze(0)) #the unsqueeze converts the scalar y to a 1D tensor\n",
        "\n",
        "    return loss\n",
        "\n",
        "\n",
        "# Keep track of losses for plotting\n",
        "current_loss = 0\n",
        "current_cross_entropy = 0\n",
        "all_losses_train = []\n",
        "all_losses_test = []\n",
        "\n",
        "#names is your dataset in python dictionary form. Keys are languages and values are list of words belonging to that language\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount = True)\n",
        "with open('/content/drive/My Drive/names.json', 'r') as fp:\n",
        "    names = json.load(fp)\n",
        "\n",
        "list_of_pairs = get_xy_pairs(names)\n",
        "train_x, train_y, test_x, test_y = create_train_and_test_set(list_of_pairs)\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    train(train_x, train_y)\n",
        "    current_loss = test(train_x, train_y)\n",
        "    all_losses_train.append(current_loss)\n",
        "    current_loss = test(test_x, test_y)\n",
        "    all_losses_test.append(current_loss)\n",
        "\n",
        "    \n",
        "\n",
        "#saving your model\n",
        "torch.save(rnn, 'rnn.pt')"
      ],
      "metadata": {
        "id": "FSM_dhviiJ9D",
        "outputId": "9b0c44f8-951d-4b57-c638-48f61a231a68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:48: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instructions: This notebook is intended to for answering the questions in the HW3_Programming pdf. You will answer in the space provided."
      ],
      "metadata": {
        "id": "WrZhGNVU_KHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1) (2 points) Plot the Cross-entropy vs. epoch curve for both the training data and the testing data. Do you see over-fitting?"
      ],
      "metadata": {
        "id": "vSIog697_Yj5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FF0riaCN9Fzr",
        "outputId": "85748833-d0ff-48b7-dcfc-7cc7d2704542",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f4416bf0c50>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "#TODO plot\n",
        "\n",
        "plt.plot(all_losses_train, label = 'train')\n",
        "plt.plot(all_losses_test, label = 'test')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2) (4 points) Plot a confusion matrix for the test set for all the 18 languages.  What can you infer from it? What languages are confused the most?"
      ],
      "metadata": {
        "id": "TNI1BpWD_fjJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO, your plot and some discussion on the plot\n",
        "\n"
      ],
      "metadata": {
        "id": "8GOW9Ve4AQ-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(3) (4 points) So far you randomly split the dataset uniformly at random into an 80:20 partition. However, this could mean unbalanced training data since all languages might not have the same number of names in the training set. This could in turn adversely affect performance. Could you think of some way of mitigating this? Can you choose to split the dataset more intelligently? Implement your idea (you are free to use existing functions from scikit-learn or any other python library for this) and report differences in accuracy with your new method over the previous uniformly at random partition."
      ],
      "metadata": {
        "id": "YOzAGfweAWld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO, your answer"
      ],
      "metadata": {
        "id": "Qm-qRW0xAViE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}